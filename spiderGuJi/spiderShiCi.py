# encoding:utf-8
"""
è¯—è¯åœ°å€ï¼šhttps://gj.zdic.net/list.php?caid=47
"""

import json, requests, time, os, pickle
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED, ALL_COMPLETED, as_completed

SAVE_DIR = "" or os.getcwd()


class SpiderShiCi:
    def __init__(self):
        self.num = 0
        pass

    def get_category_first(self):
        """
        ç¬¬ä¸€çº§ç›®å½•ï¼š é›†éƒ¨-ã€‹è¯ï¼Œ æ¥šè¾...
        """
        url = "https://gj.zdic.net/"
        html_str = requests.get(url)
        if html_str.status_code == 200:
            html_val = html_str.text
            result_beautiful = BeautifulSoup(html_val, 'html.parser')
            # è·å–ä¸€çº§å’ŒğŸ§å†…å®¹
            box_obj = result_beautiful.select('div[id=gj_dh_z] div')
            # box_obj.pop()
            res = {}
            if len(box_obj) != 8:
                raise "è·å–ç±»åˆ«æœ‰é”™è¯¯"
            for i in range(0, 4):
                cate_name = box_obj[i].text.strip()
                children_list = box_obj[i+4].select('ul li a')
                res.update({
                    cate_name: [{'name': i.text.strip(), "href": 'https://gj.zdic.net' + i.attrs.get('href')} for i in children_list],
                })
                # break
            return res
        else:
            raise "{} è·å–æ•°æ®å¤±è´¥".format(url)

    def _get_cur_page_data(self, dl_obj):
        """
        å½“å‰ä¼ å…¥çš„å¯¹è±¡æ˜¯dl æ ‡ç­¾
        """
        cir_list = dl_obj.select('dd ul li b a')
        return [{'name': i.text.strip(), 'href': "https://gj.zdic.net/"+i.attrs.get('href')} for i in cir_list if i.attrs.get('href')]

    def _get_end_flag(self, cur_num,end_num, range=9):
        return cur_num + range == end_num

    def _write_file(self, f_name, url):
        """å†™æ–‡ä»¶æ“ä½œ"""
        if url is None:
            with open(f_name, 'w', encoding='utf-8') as f:
                f.write('')
            return 'success'
        try:
            html_str = requests.get(url)
        except:
            print('e')

        self.num += 1
        if os.path.exists(f_name):
            print('{} {} æ–‡ä»¶å·²ç»å­˜åœ¨'.format(self.num, f_name))
            return 'success'

        if html_str.status_code == 200:
            # è¯·æ±‚æˆåŠŸ
            html_val = html_str.text
            result_beautiful = BeautifulSoup(html_val, 'html.parser')

            content_res = result_beautiful.select('div[id="snr2"]')
            if content_res:
                print('**** {} {} å†™å…¥æˆåŠŸ'.format(self.num, f_name))
                with open(f_name, 'w', encoding='utf-8') as f:
                    f.writelines(content_res[0].text.strip())
            return 'sucess'
        else:
            print("{}: å†™æ–‡ä»¶å¤±è´¥å¤±è´¥".format(url))
            return 'fail'

    def loop_write_file(self, dir_name, cur_val_list,file_list):

        for cur_val in cur_val_list:
            if cur_val.get('children'):
                self.loop_write_file(os.path.join(dir_name, cur_val['name']), cur_val.get('children'), file_list)
            else:
                if os.path.exists(dir_name) == False:
                    os.makedirs(dir_name)

                jieshao_con = cur_val.get('jieShao')
                file_name = os.path.join(dir_name, "{}.txt".format(cur_val['name'].replace('/', '_')))
                if jieshao_con:
                    jieshao_ff = os.path.join(dir_name, 'ä»‹ç».txt')
                    if os.path.exists(jieshao_ff) == False:
                        with open(jieshao_ff, 'w', encoding='utf-8') as f:
                            f.write(cur_val['jieShao'])

                # self._write_file(file_name, cur_val['href'])
                file_list.append([file_name, cur_val['href']])

    def get_category_second(self, url):
        """
        ç¬¬äºŒçº§ç›®å½•ï¼š æ¥šè¾-ã€‹æ–‡å¿ƒé›•é¾™...
        # æ¯é¡µ10ä¸ª
        """
        html_str = requests.get(url)
        if html_str.status_code == 200:
            html_val = html_str.text
            result_beautiful = BeautifulSoup(html_val, 'html.parser')
            total_cir_list = []
            # åˆ¤æ–­æœ‰æ²¡æœ‰åˆ†é¡µæ•°æ®
            box_obj = result_beautiful.select('div[id=list_d_1] dl')
            if box_obj is None:
                return []
            box_obj = box_obj[0]
            total_cir_list.extend(self._get_cur_page_data(box_obj))
            page_nav_list = box_obj.select('div[class=pagenav] div[class=p_bar] a')
            if page_nav_list:
                cur_page_nav = box_obj.select('div[class=pagenav] div[class=p_bar] a[class=p_curpage]')[0].text.strip()

                if cur_page_nav == page_nav_list[-1].text.strip():
                    return total_cir_list

                # æ‰¾å‡ºå½“å‰é¡µæ‰€åœ¨çš„ç´¢å¼•
                # <a class="p_curpage">2</a>
                cur_page_idx = [idx for idx, i in enumerate(page_nav_list) if "p_curpage" in i.attrs.get('class', [])]
                cur_url = page_nav_list[cur_page_idx[0] + 1].attrs.get('href')
                total_cir_list.extend(self.get_category_second(cur_url))
            return total_cir_list
        else:
            raise ("{} äºŒçº§ç›®å½•è¯·æ±‚å¤±è´¥".format(url))

    def get_category_third(self, url):
        """
        è·å–æ–‡ä»¶è·¯å¾„ï¼š
        ç¬å±±è®°--ã€‹â— ç¬¬ä¸€å›ã€€å¯å®¶å„¿è¯»ä¹¦è´»ç¬‘ã€€ç‰æ°å­å‡ºå±±æ±‚å,â— ç¬¬äºŒå›ã€€èµ‚æœ¬å®˜æ‹™è¡Œé“æ‰‡å­ã€€æƒ©åœŸæ¶ç—›æ‰“ä¸éœ¸ç‹....
        """
        html_str = requests.get(url)
        if html_str.status_code == 200:
            html_val = html_str.text
            result_beautiful = BeautifulSoup(html_val, 'html.parser')
            # è·å–ä¸€çº§å’ŒğŸ§å†…å®¹
            box_obj = result_beautiful.select('div[id=ml_2] div[class=mls] li a')

            # è·å–ä»‹ç»æ•°æ®
            jieshao = ''
            if result_beautiful.select('div[id=jj_2]'):
                jieshao = result_beautiful.select('div[id=jj_2]')[0].text.strip()

            return [{'name': i.text.strip().replace(' ', '-'), "jieShao":jieshao, 'href': 'https://gj.zdic.net/' + i.attrs.get('href')} for i in box_obj]
        else:
            raise "{} ä¸‰çº§ç›®å½•è·å–æ•°æ®å¤±è´¥".format(url)


    def spider_category(self):
        """ä¸‹è½½æ‰€æœ‰çš„ç›®å½•"""
        ff = "spiderShiCi.pkl"
        if os.path.exists(ff):
            with open(ff, 'rb') as f:
                data = pickle.load(f)
        else:
            # ç¬¬ä¸€çº§ æ•°æ®
            res = self.get_category_first()
            #
            # è·å–ç¬¬äºŒçº§ç±»ç›®
            t1 = time.time()
            for _, first_cates in res.items():
                for first_cate in first_cates:
                    res2 = self.get_category_second(first_cate['href'])
                    first_cate.update({'children': res2})

            print('è·å–ä¸€çº§å’ŒäºŒçº§çš„åˆ†ç±»è€—æ—¶ï¼š{}'.format(time.time() - t1))

            t2 = time.time()

            # è·å–ä¸‰çº§ç›®å½•å’Œå››çº§ç›®å½•
            for _, firat_cates in res.items():
                for first_cate in firat_cates:
                    if 'name' not in first_cate or 'children' not in first_cate:
                        print('{} ç›®å½•ä¸‹æ˜¯ç©º'.format(_))
                        continue
                    name = first_cate['name']
                    print('{}-{}-å­˜åœ¨ {} æ¡'.format(_, name, len(first_cate['children'])))
                    for son in first_cate['children']:
                        res3 = self.get_category_third(son['href'])
                        print('{}-{}-{} å­˜åœ¨ {} æ¡'.format(_, name, son['name'], len(res3)))
                        son.update({'children': res3})
                        print('res')
                    print('\n' * 3)
            print('è·å–æ–‡æœ¬è¯¦æƒ…æ–‡ä»¶è€—æ—¶ï¼š {}'.format(time.time() - t2))
            # print(res)
            file_list = []
            # å¼€å§‹å†™æ“ä½œ
            for k, v in res.items():
                self.loop_write_file(os.path.join(SAVE_DIR, k), v, file_list)
            with open(ff, 'wb') as f:
                pickle.dump(file_list, f)
            data = file_list
        return data

    def main(self):
        """
        æ•°æ®ç”Ÿæˆ
        """
        file_list = self.spider_category()
        print('ä¸€å…±éœ€è¦ä¸‹è½½{}ä¸ª'.format(len(file_list)))
        # å¤šçº¿ç¨‹ä¸‹è½½æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=5) as t:
            tt_list = [t.submit(self._write_file, i[0], i[1]) for i in file_list]
            wait(tt_list, return_when=FIRST_COMPLETED)
            for tt in as_completed(tt_list):
                data = tt.result()
                print("thread==", data)




obj = SpiderShiCi()
obj.main()
