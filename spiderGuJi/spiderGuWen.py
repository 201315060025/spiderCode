# encoding: utf-8
"""
url: https://so.gushiwen.cn/guwen/
"""
import json, requests, time, os, pickle
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

SAVE_DIR = "" or os.getcwd()


def get_category_first():
    """
    è·å–ä¼°è®¡çš„å¤§ç±»
    è¿”å›æ ¼å¼ï¼š
    {
    "ç»éƒ¨"ï¼š[{"name": "", 'url": ""}]
    }
    """
    url = "https://so.gushiwen.cn/guwen/"
    html_str = requests.get(url)
    res = {}
    if html_str.status_code == 200:
        # è¯·æ±‚æˆåŠŸ
        html_val = html_str.text
        result_beautiful = BeautifulSoup(html_val, 'html.parser')

        son2_list = result_beautiful.find_all('div', class_='son2')
        son2_list.pop(0)

        for son2 in son2_list:
            # è·å–å¤§ç±»ä¸‹çš„æ¯ä¸ªå°ç±»ï¼š --ã€‹ ç»éƒ¨ï¼šæ˜“ç±» ä¹¦ç±» è¯—ç±» ç¤¼ç±»...
            category = son2.text.split("ï¼š")[0].strip()
            son_list = son2.select('[class~=sright] a')

            res.update({
                category: [{'name': son.text.strip(), 'href': 'https://so.gushiwen.cn' + son.attrs['href']} for son in son_list]
            })
        return res
    else:
        raise "è¯·æ±‚ä¸»ç±»ç›®å¤±è´¥ï¼"



def get_category_second(url):
    """
    è·å–æ˜“ç±»ä¸‹é¢æ‰€æœ‰çš„ç±»åˆ«
    ç»éƒ¨ï¼šæ˜“ç±» --> å‘¨æ˜“(å…ˆç§¦) æ˜“ä¼ (å…ˆç§¦) å­å¤æ˜“ä¼ (å…ˆç§¦)
    """
    html_str = requests.get(url)
    res = []
    if html_str.status_code == 200:
        # è¯·æ±‚æˆåŠŸ
        html_val = html_str.text
        result_beautiful = BeautifulSoup(html_val, 'html.parser')

        son2_list = result_beautiful.select('div[class="typecont"] span')

        return [{'name': i.text, 'href': 'https://so.gushiwen.cn' + i.next.attrs['href']} for i in son2_list]
    else:
        raise "è¯·æ±‚ğŸ§ç›®æ ‡å¤±è´¥"

def get_category_third(url):
    """è·å–ä¸‰çº§ç›®å½•æˆ–å››çº§åˆ«ï¼Œæœ€ç»ˆæ–‡ä»¶è·¯å¾„"""
    html_str = requests.get(url)
    res = []
    if html_str.status_code == 200:
        # è¯·æ±‚æˆåŠŸ
        html_val = html_str.text
        result_beautiful = BeautifulSoup(html_val, 'html.parser')

        bookcont_list = result_beautiful.select('div[class="sons"] div[class="bookcont"]')
        book_detail_res = []
        for bookcont in bookcont_list:
            bookcont_parent = bookcont.find('div', class_='bookMl')
            if bookcont_parent:
                parent_name = bookcont_parent.text.strip()
                book_detail_list = bookcont.select('div span a')
                book_detail_res.extend([{'name': i.text.strip(), 'href': i.attrs.get('href'), 'parent_name': parent_name} for i in book_detail_list])
            else:
                book_detail_list = bookcont.select('div ul span a')
                book_detail_res.extend([{'name': i.text.strip(), 'href': i.attrs.get('href')} for i in book_detail_list])
        return book_detail_res
    else:
        raise "è¯·æ±‚ğŸ§ç›®æ ‡å¤±è´¥"

number = 0
def write_file(f_name, url):
    """å†™æ–‡ä»¶æ“ä½œ"""
    if url is None:
        with open(f_name, 'w', encoding='utf-8') as f:
            f.write('')
        return 'sucess'
    try:
        html_str = requests.get(url)
    except:
        print('e')

    global number
    number += 1
    if os.path.exists(f_name):
        print('{} {} æ–‡ä»¶å·²ç»å­˜åœ¨'.format(number, f_name))
        return 'sucess'

    if html_str.status_code == 200:
        # è¯·æ±‚æˆåŠŸ
        html_val = html_str.text
        result_beautiful = BeautifulSoup(html_val, 'html.parser')

        content_list = result_beautiful.select('div[class="contson"] p')
        content_res = []
        for bookcont in content_list:
            if bookcont.text.strip():
                content_res.append(bookcont.text.strip()+"\n")
            pass
        print('**** {} {} å†™å…¥æˆåŠŸ'.format(number, f_name))
        with open(f_name, 'w', encoding='utf-8') as f:
            f.writelines(content_res)
        return 'sucess'
    else:
        print("{} {}: å†™æ–‡ä»¶å¤±è´¥å¤±è´¥".format(number, url))
        return 'fail'


def loop_write_file(dir_name, cur_val_list, file_list):

    for cur_val in cur_val_list:
        if cur_val.get('children'):
            loop_write_file(os.path.join(dir_name, cur_val['name']), cur_val.get('children'), file_list)
        else:
            if os.path.exists(dir_name) == False:
                os.makedirs(dir_name)
            file_name = os.path.join(dir_name, "{}.txt".format(cur_val['name'].replace('/','_')))
            if cur_val.get('parent_name'):
                tmp_name = os.path.join(dir_name, cur_val.get('parent_name'))
                if os.path.exists(tmp_name) == False: os.makedirs(tmp_name)
                file_name = os.path.join(tmp_name, "{}.txt".format(cur_val['name'].replace('/','_')))
            # write_file(file_name, cur_val['href'])
            file_list.append([file_name, cur_val['href']])


def spider_cate():
    """"""
    ff = "spiderShiCi.pkl"
    if os.path.exists(ff):
        with open(ff, 'rb') as f:
            data = pickle.load(f)
    else:
        # è·å–ç¬¬ä¸€çº§ç±»ç›®
        res = get_category_first()
        # è·å–ç¬¬äºŒçº§ç±»ç›®
        t1 = time.time()
        for _, first_cates in res.items():
            for first_cate in first_cates:
                res2 = get_category_second(first_cate['href'])
                first_cate.update({'children': res2})

        print('è·å–ä¸€çº§å’ŒäºŒçº§çš„åˆ†ç±»è€—æ—¶ï¼š{}'.format(time.time() - t1))
        t2 = time.time()

        # è·å–ä¸‰çº§ç›®å½•å’Œå››çº§ç›®å½•
        for _, firat_cates in res.items():
            for first_cate in firat_cates:
                if 'name' not in first_cate or 'children' not in first_cate:
                    print('{} ç›®å½•ä¸‹æ˜¯ç©º'.format(_))
                    continue
                name = first_cate['name']
                print('{}-{}-å­˜åœ¨ {} æ¡'.format(_, name, len(first_cate['children'])))
                for son in first_cate['children']:
                    res3 = get_category_third(son['href'])
                    print('{}-{}-{} å­˜åœ¨ {} æ¡'.format(_, name, son['name'], len(res3)))
                    son.update({'children': res3})
                    print('res')
                print('\n' * 3)
        print('è·å–æ–‡æœ¬è¯¦æƒ…æ–‡ä»¶è€—æ—¶ï¼š {}'.format(time.time() - t2))

        # å¼€å§‹å†™æ“ä½œ
        file_list = []
        for k, v in res.items():
            loop_write_file(os.path.join(SAVE_DIR, k), v, file_list)
        with open(ff, 'wb') as f:
            pickle.dump(file_list, f)
        data = file_list
    return data


def main():

    file_list = spider_cate()
    print('ä¸€å…±éœ€è¦ä¸‹è½½{}ä¸ª'.format(len(file_list)))

    # å¤šçº¿ç¨‹ä¸‹è½½æ–‡ä»¶
    with ThreadPoolExecutor(max_workers=5) as t:
        tt_list = [t.submit(write_file, i[0], i[1]) for i in file_list]
        for tt in as_completed(tt_list):
            data = tt.result()
            print("thread==", data)

main()



